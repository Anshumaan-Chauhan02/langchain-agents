{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "391ea3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm =  ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f774b522",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory with RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316e62e4",
   "metadata": {},
   "source": [
    "`ConversationBufferMemory` was deprecated, instead we will use `RunnableWithMessageHistory` class to implement the same functionality. We will use LangChain Expression Language (LCEL) and for this we need to define our prompt template and LLM components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "706347f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate, \n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    ChatPromptTemplate\n",
    ")\n",
    "\n",
    "system_prompt = \"You are a helpful assistant called Zeta.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd0cb375",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = prompt_template | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f08d16",
   "metadata": {},
   "source": [
    "Our RunnableWithMessageHistory requires our pipeline to be wrapped in a RunnableWithMessageHistory object. This object requires a few input parameters. One of those is get_session_history, which requires a function that returns a ChatMessageHistory object based on a session ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4346340b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "chat_map = {}\n",
    "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbc5171",
   "metadata": {},
   "source": [
    "We also need to tell our runnable which variable name to use for the chat history (ie history) and which to use for the user's query (ie query)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a71bf623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07500bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<think>\\nOkay, so I'm trying to figure out how to solve this equation: 3x + 5 = 20. Hmm, let me think about it step by step.\\n\\nFirst, I need to isolate the variable x. That means getting x by itself on one side of the equation. Right now, there's a 3 multiplied by x and then we add 5. So maybe I should start by subtracting 5 from both sides to get rid of that constant term on the left.\\n\\nLet me write that down: 3x + 5 - 5 = 20 - 5. Simplifying both sides, that becomes 3x = 15. Okay, so now we have 3 times x equals 15. To solve for x, I need to get rid of the coefficient 3 in front of x.\\n\\nHow do I do that? Well, if something is multiplied by a number and you want to find out what it was before, you divide both sides by that number. So in this case, I should divide both sides by 3.\\n\\nLet me do that: (3x)/3 = 15/3. Simplifying both sides, the left side becomes x because 3 divided by 3 is 1, and on the right side, 15 divided by 3 is 5. So now we have x = 5.\\n\\nWait a minute, let me check if that makes sense. If I plug x = 5 back into the original equation, does it hold true? Let's see: 3*(5) + 5 equals 20. That would be 15 + 5, which is indeed 20. So yes, x = 5 is the correct solution.\\n\\nI think that makes sense. I first subtracted 5 to isolate the term with x, then divided by 3 to solve for x. And checking it by plugging back in confirms that it's right. I don't see any mistakes in my steps, so I'm confident that x = 5 is the solution.\\n</think>\\n\\nTo solve the equation \\\\(3x + 5 = 20\\\\), follow these steps:\\n\\n1. **Subtract 5 from both sides** to isolate the term with the variable:\\n   \\\\[\\n   3x + 5 - 5 = 20 - 5\\n   \\\\]\\n   Simplifying, we get:\\n   \\\\[\\n   3x = 15\\n   \\\\]\\n\\n2. **Divide both sides by 3** to solve for \\\\(x\\\\):\\n   \\\\[\\n   \\\\frac{3x}{3} = \\\\frac{15}{3}\\n   \\\\]\\n   Simplifying, we find:\\n   \\\\[\\n   x = 5\\n   \\\\]\\n\\n3. **Verify the solution** by substituting \\\\(x = 5\\\\) back into the original equation:\\n   \\\\[\\n   3(5) + 5 = 20\\n   \\\\]\\n   This simplifies to:\\n   \\\\[\\n   15 + 5 = 20\\n   \\\\]\\n   Which is true, confirming that \\\\(x = 5\\\\) is the correct solution.\\n\\n**Final Answer:** \\\\(x = 5\\\\).\", additional_kwargs={}, response_metadata={'model': 'deepseek-r1:1.5b', 'created_at': '2025-05-18T14:21:17.21387073Z', 'done': True, 'done_reason': 'stop', 'total_duration': 7899483600, 'load_duration': 4405879573, 'prompt_eval_count': 18, 'prompt_eval_duration': 354618615, 'eval_count': 675, 'eval_duration': 3133473239, 'model_name': 'deepseek-r1:1.5b'}, id='run--551943bd-39d7-479e-b84b-68451ddadf1c-0', usage_metadata={'input_tokens': 18, 'output_tokens': 675, 'total_tokens': 693})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is James\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "287feae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nAlright, so I\\'m trying to figure out what my name is. Hmm, where do I even start with this? Maybe it\\'s something simple like \"James\"? That seems familiar from when I was younger. But wait, could there be more to it?\\n\\nLet me think about the context in which I\\'m asking this question. It just says, \"What is my name again?\" So, maybe it\\'s a play on words or something else. Could it be related to math? Like, if I solve an equation and get a number as my name... But that doesn\\'t seem quite right.\\n\\nOh, maybe it\\'s about the letters in my name. If I spell out my name with letters, could that form another word or phrase? For example, \"James\" has J-A-M-E-S. Is there any hidden meaning or connection to those letters?\\n\\nOr perhaps it\\'s a riddle or a pun. Sometimes names are given in a way that\\'s tricky or uses double meanings. Maybe \"James\" is being used metaphorically or literally in a different context.\\n\\nI could also think about famous people with similar names, but I\\'m not sure if that helps. Maybe it\\'s something from a book or a movie where the name plays a role. But without more information, it\\'s hard to pinpoint exactly what my name refers to.\\n\\nWait, maybe it\\'s related to technology or science? Like, if I were a computer programmer or an engineer, would their names have any connection to \"James\"? Probably not directly, but perhaps there are references in the field that use \"James\" as part of their work.\\n\\nAlternatively, could it be about my achievements or accomplishments? If someone is known for something named after me, maybe that\\'s what they\\'re referring to. But without specific details, it\\'s challenging to say.\\n\\nI guess another approach would be to think about how names are often abbreviated or have symbolic meanings in different cultures or languages. Maybe \"James\" has a particular significance in the way it\\'s pronounced or written.\\n\\nBut honestly, I\\'m not sure if I\\'m overcomplicating this. It seems like the most straightforward answer is that my name is James. Unless there\\'s something more to it that I\\'m missing, which would make the question more interesting and thought-provoking.\\n</think>\\n\\nMy name is James.', additional_kwargs={}, response_metadata={'model': 'deepseek-r1:1.5b', 'created_at': '2025-05-18T14:21:53.475986888Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2729906097, 'load_duration': 90370950, 'prompt_eval_count': 268, 'prompt_eval_duration': 281840706, 'eval_count': 472, 'eval_duration': 2351176980, 'model_name': 'deepseek-r1:1.5b'}, id='run--5d151a91-ab82-4154-b183-7d43903ace51-0', usage_metadata={'input_tokens': 268, 'output_tokens': 472, 'total_tokens': 740})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"What is my name again?\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7888fd3f",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory with RunnableWithMessageHistory\n",
    "\n",
    "The ConversationBufferWindowMemory type is similar to ConversationBufferMemory, but only keeps track of the last k messages. There are a few reasons why we would want to keep only the last k messages:\n",
    "\n",
    "More messages mean more tokens are sent with each request, more tokens increases latency and cost.\n",
    "\n",
    "LLMs tend to perform worse when given more tokens, making them more likely to deviate from instructions, hallucinate, or \"forget\" information provided to them. Conciseness is key to high performing LLMs.\n",
    "\n",
    "If we keep all messages we will eventually hit the LLM's context window limit, by adding a window size k we can ensure we never hit this limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcdb714c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class BufferWindowMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    # Implements the methods from BaseChatMessageHistory\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    k: int = Field(default_factory=int)\n",
    "\n",
    "    def __init__(self, k: int):\n",
    "        super().__init__(k=k)\n",
    "        print(f\"Initializing BufferWindowMessageHistory with k={k}\")\n",
    "    \n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        self.messages.extend(messages)\n",
    "        self.messages = self.messages[-self.k:]\n",
    "    \n",
    "    def clear(self) -> None:\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7835e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "def get_chat_history(session_id: str, k: int=4)  -> BufferWindowMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        chat_map[session_id] = BufferWindowMessageHistory(k=k)\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1f2aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\"\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"k\",\n",
    "            annotation=int,\n",
    "            name=\"k\",\n",
    "            description=\"The number of messages to keep in the history\",\n",
    "            default=4\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6f461b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BufferWindowMessageHistory with k=4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<think>\\nOkay, so I'm trying to figure out how to solve this equation: 3x + 5 = 20. Hmm, let me think about it step by step.\\n\\nFirst, I remember that when solving equations, the goal is to isolate the variable, which in this case is x. So, I need to get x by itself on one side of the equation.\\n\\nLooking at the equation, there are two operations being performed on x: multiplication and addition. Specifically, 3x means x is multiplied by 3, and then 5 is added to that product. To reverse these operations, I should do them in the opposite order. That usually means dealing with the addition first before handling the multiplication.\\n\\nSo, let me start by subtracting 5 from both sides of the equation. If I subtract 5 from the left side, it becomes 3x + 5 - 5, which simplifies to 3x. On the right side, subtracting 5 gives me 20 - 5, which is 15. So now my equation looks like this: 3x = 15.\\n\\nNext, I need to get rid of that 3 that's multiplied by x. To do that, I should divide both sides of the equation by 3. Dividing the left side by 3 gives me (3x)/3, which simplifies to just x. On the right side, dividing 15 by 3 results in 5. So now my equation is simplified to x = 5.\\n\\nWait a second, let me check if that makes sense. If I plug x = 5 back into the original equation, does it hold true? Let's see: 3*(5) + 5 equals 15 + 5, which is 20. And the right side of the equation is also 20. So yes, both sides are equal when x is 5. That means my solution is correct.\\n\\nI think that covers it. I isolated x by first undoing the addition and then dealing with the multiplication. It's important to remember the order of operations in reverse when solving equations because you want to do the opposite operations in the right sequence.\\n</think>\\n\\nTo solve the equation \\\\(3x + 5 = 20\\\\), follow these steps:\\n\\n1. **Subtract 5 from both sides** to isolate the term with the variable:\\n   \\\\[\\n   3x + 5 - 5 = 20 - 5\\n   \\\\]\\n   Simplifying this gives:\\n   \\\\[\\n   3x = 15\\n   \\\\]\\n\\n2. **Divide both sides by 3** to solve for \\\\(x\\\\):\\n   \\\\[\\n   \\\\frac{3x}{3} = \\\\frac{15}{3}\\n   \\\\]\\n   Simplifying this results in:\\n   \\\\[\\n   x = 5\\n   \\\\]\\n\\nTo verify the solution, substitute \\\\(x = 5\\\\) back into the original equation:\\n\\\\[\\n3(5) + 5 = 20\\n\\\\]\\nSimplifying:\\n\\\\[\\n15 + 5 = 20\\n\\\\]\\nWhich confirms that \\\\(x = 5\\\\) is the correct solution.\\n\\n**Final Answer:** \\\\(x = 5\\\\)\", additional_kwargs={}, response_metadata={'model': 'deepseek-r1:1.5b', 'created_at': '2025-05-18T17:16:35.578987805Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5593085537, 'load_duration': 2256950485, 'prompt_eval_count': 18, 'prompt_eval_duration': 297305364, 'eval_count': 681, 'eval_duration': 3036318276, 'model_name': 'deepseek-r1:1.5b'}, id='run--e27a6713-8b2b-4e0d-9cd7-a14b3bbf671a-0', usage_metadata={'input_tokens': 18, 'output_tokens': 681, 'total_tokens': 699})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is Josh\"},\n",
    "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9348e7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_k4\"].clear()  # clear the history\n",
    "\n",
    "# manually insert history\n",
    "chat_map[\"id_k4\"].add_user_message(\"Hi, my name is Josh\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"I'm an AI model called Zeta.\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"Very cool!\")\n",
    "\n",
    "chat_map[\"id_k4\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a9adb14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\n\\n</think>\\n\\nI donâ€™t have access to internal company information. For more details about DeepSeek, please visit the official website.', additional_kwargs={}, response_metadata={'model': 'deepseek-r1:1.5b', 'created_at': '2025-05-18T17:17:13.220213749Z', 'done': True, 'done_reason': 'stop', 'total_duration': 171396554, 'load_duration': 13198835, 'prompt_eval_count': 61, 'prompt_eval_duration': 12940668, 'eval_count': 28, 'eval_duration': 136102283, 'model_name': 'deepseek-r1:1.5b'}, id='run--186f19e1-c300-419d-ac62-e2ab9c5790b0-0', usage_metadata={'input_tokens': 61, 'output_tokens': 28, 'total_tokens': 89})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"what is my name again?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8678df6d",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory with RunnableWithMessageHistory\n",
    "\n",
    "This memory type keeps track of a summary of the conversation rather than the entire conversation. This is useful for long conversations where we don't need to keep track of the entire conversation, but we do want to keep some thread of the full conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c467416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "class ConversationSummaryMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseModel] = Field(default_factory=list)\n",
    "    llm: ChatOllama = Field(default_factory=ChatOllama)\n",
    "\n",
    "    def __init__(self, llm: ChatOllama):\n",
    "        super().__init__(llm=llm)\n",
    "    \n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        self.messages.extend(messages)\n",
    "        summary_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                SystemMessagePromptTemplate.from_template(\n",
    "                    \"Given the existing conversation summary and the new messages, \"\n",
    "                    \"generate a new summary of the conversation. Ensuring to maintain \"\n",
    "                    \"as much relevant information as possible BUT keep the summary \"\n",
    "                    \"concise and no more than a short paragraph in length.\"\n",
    "                ),\n",
    "                HumanMessagePromptTemplate.from_template(\n",
    "                    \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
    "                    \"New messages:\\n{messages}\"\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        new_summary = self.llm.invoke(\n",
    "            summary_prompt.format_messages(existing_summary=self.messages, messages=messages)\n",
    "        )\n",
    "        self.messages = [SystemMessage(content=new_summary.content)]\n",
    "    \n",
    "    def clear(self) -> None:\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f83e07e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "def get_chat_history(session_id: str, llm: ChatOllama) -> ConversationSummaryMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        chat_map[session_id] = ConversationSummaryMessageHistory(llm=llm)\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bed3633",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"llm\",\n",
    "            annotation=ChatOllama,\n",
    "            name=\"LLM\",\n",
    "            description=\"The LLM to use for the conversation summary\",\n",
    "            default=llm,\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f7c3cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"<think>\\nOkay, so I'm trying to figure out how to solve this equation: 3x + 5 = 20. Hmm, let me think about it step by step.\\n\\nFirst, I remember that when solving equations, the goal is to isolate the variable, which in this case is x. So, I need to get x by itself on one side of the equation.\\n\\nLooking at the equation, there are two operations being performed on x: multiplication and addition. Specifically, 3x means x is multiplied by 3, and then 5 is added to that product. To reverse these operations, I should do them in the opposite order. That usually means dealing with the addition first before handling the multiplication.\\n\\nSo, let me start by subtracting 5 from both sides of the equation. If I subtract 5 from the left side, it becomes 3x + 5 - 5, which simplifies to 3x. On the right side, subtracting 5 gives me 20 - 5, which is 15. So now my equation looks like this: 3x = 15.\\n\\nNext, I need to get rid of that 3 that's multiplied by x. To do that, I should divide both sides of the equation by 3. Dividing the left side by 3 gives me (3x)/3, which simplifies to just x. On the right side, dividing 15 by 3 results in 5. So now my equation is simplified to x = 5.\\n\\nWait a second, let me check if that makes sense. If I plug x = 5 back into the original equation, does it hold true? Let's see: 3*(5) + 5 equals 15 + 5, which is 20. And the right side of the equation is also 20. So yes, both sides are equal when x is 5. That means my solution is correct.\\n\\nI think that covers it. I isolated x by first undoing the addition and then dealing with the multiplication. It's important to remember the order of operations in reverse when solving equations because you want to do the opposite operations in the right sequence.\\n</think>\\n\\nTo solve the equation \\\\(3x + 5 = 20\\\\), follow these steps:\\n\\n1. **Subtract 5 from both sides** to isolate the term with the variable:\\n   \\\\[\\n   3x + 5 - 5 = 20 - 5\\n   \\\\]\\n   Simplifying this gives:\\n   \\\\[\\n   3x = 15\\n   \\\\]\\n\\n2. **Divide both sides by 3** to solve for \\\\(x\\\\):\\n   \\\\[\\n   \\\\frac{3x}{3} = \\\\frac{15}{3}\\n   \\\\]\\n   Simplifying this results in:\\n   \\\\[\\n   x = 5\\n   \\\\]\\n\\nTo verify the solution, substitute \\\\(x = 5\\\\) back into the original equation:\\n\\\\[\\n3(5) + 5 = 20\\n\\\\]\\nSimplifying:\\n\\\\[\\n15 + 5 = 20\\n\\\\]\\nWhich confirms that \\\\(x = 5\\\\) is the correct solution.\\n\\n**Final Answer:** \\\\(x = 5\\\\)\", additional_kwargs={}, response_metadata={'model': 'deepseek-r1:1.5b', 'created_at': '2025-05-18T17:28:58.86684595Z', 'done': True, 'done_reason': 'stop', 'total_duration': 6547995018, 'load_duration': 3262761138, 'prompt_eval_count': 18, 'prompt_eval_duration': 195054922, 'eval_count': 681, 'eval_duration': 3088422691, 'model_name': 'deepseek-r1:1.5b'}, id='run--77d0bdae-8e38-4a27-93a4-1ba537bf2c25-0', usage_metadata={'input_tokens': 18, 'output_tokens': 681, 'total_tokens': 699})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is Josh\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef52d72c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='<think>\\n\\n**Final Answer:** \\\\\\\\(x = 5\\\\\\\\)', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_123\"].messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fa34ca",
   "metadata": {},
   "source": [
    "## ConversationSummaryBufferMemory with RunnableWithMessageHistory\n",
    "\n",
    "Our final memory type acts as a combination of ConversationSummaryMemory and ConversationBufferMemory. It keeps the buffer for the conversation up until the previous n tokens, anything beyond that limit is summarized then dropped from the buffer. Producing something like:\n",
    "\n",
    "```text\n",
    "# ~~ a summary of previous interactions\n",
    "The user named Josh introduced himself and the AI responded, introducing itself as an AI model called Zeta.\n",
    "Josh then said he was researching the different types of conversational memory and Zeta asked for some\n",
    "examples.\n",
    "# ~~ the most recent messages\n",
    "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
    "AI: That's interesting, what's the difference?\n",
    "Human: Buffer memory just stores the entire conversation\n",
    "AI: That makes sense, what about ConversationBufferWindowMemory?\n",
    "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
    "AI: Very cool!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82c83ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationSummaryBufferMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    llm: ChatOllama = Field(default_factory=ChatOllama)\n",
    "    k: int = Field(default_factory=int)\n",
    "\n",
    "    def __init__(self, llm: ChatOllama, k: int):\n",
    "        super().__init__(llm=llm, k=k)\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        existing_summary = None\n",
    "        old_messages = None\n",
    "        # see if we already have a summary message\n",
    "        if len(self.messages) > 0 and isinstance(self.messages[0], SystemMessage):\n",
    "            print(\">> Found existing summary\")\n",
    "            existing_summary: str | None = self.messages.pop(0)\n",
    "        # add the new messages to the history\n",
    "        self.messages.extend(messages)\n",
    "        # check if we have too many messages\n",
    "        if len(self.messages) > self.k:\n",
    "            print(\n",
    "                f\">> Found {len(self.messages)} messages, dropping \"\n",
    "                f\"latest {len(self.messages) - self.k} messages.\")\n",
    "            # pull out the oldest messages...\n",
    "            old_messages = self.messages[:self.k]\n",
    "            # ...and keep only the most recent messages\n",
    "            self.messages = self.messages[-self.k:]\n",
    "        if old_messages is None:\n",
    "            print(\">> No old messages to update summary with\")\n",
    "            # if we have no old_messages, we have nothing to update in summary\n",
    "            return\n",
    "        # construct the summary chat messages\n",
    "        summary_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"Given the existing conversation summary and the new messages, \"\n",
    "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
    "                \"as much relevant information as possible BUT keep the summary \"\n",
    "                \"concise and no more than a short paragraph in length.\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
    "                \"New messages:\\n{old_messages}\"\n",
    "            )\n",
    "        ])\n",
    "        # format the messages and invoke the LLM\n",
    "        new_summary = self.llm.invoke(\n",
    "            summary_prompt.format_messages(\n",
    "                existing_summary=existing_summary,\n",
    "                old_messages=old_messages\n",
    "            )\n",
    "        )\n",
    "        print(f\">> New summary: {new_summary.content}\")\n",
    "        # prepend the new summary to the history\n",
    "        self.messages = [SystemMessage(content=new_summary.content)] + self.messages\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9f30f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "def get_chat_history(\n",
    "    session_id: str,\n",
    "    llm: ChatOllama,\n",
    "    k: int\n",
    ") -> ConversationSummaryBufferMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        chat_map[session_id] = ConversationSummaryBufferMessageHistory(llm=llm, k=k)\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1b02e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"llm\",\n",
    "            annotation=ChatOllama,\n",
    "            name=\"LLM\",\n",
    "            description=\"The LLM to use for the conversation summary\",\n",
    "            default=llm,\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"k\",\n",
    "            annotation=int,\n",
    "            name=\"k\",\n",
    "            description=\"The number of messages to keep in the history\",\n",
    "            default=4,\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "283e0866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> No old messages to update summary with\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, my name is Josh', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"<think>\\nOkay, so I'm trying to figure out how to solve this equation: 3x + 5 = 20. Hmm, let me think about it step by step.\\n\\nFirst, I remember that when solving equations, the goal is to isolate the variable, which in this case is x. So, I need to get x by itself on one side of the equation.\\n\\nLooking at the equation, there are two operations being performed on x: multiplication and addition. Specifically, 3x means x is multiplied by 3, and then 5 is added to that product. To reverse these operations, I should do them in the opposite order. That usually means dealing with the addition first before handling the multiplication.\\n\\nSo, let me start by subtracting 5 from both sides of the equation. If I subtract 5 from the left side, it becomes 3x + 5 - 5, which simplifies to 3x. On the right side, subtracting 5 gives me 20 - 5, which is 15. So now my equation looks like this: 3x = 15.\\n\\nNext, I need to get rid of that 3 that's multiplied by x. To do that, I should divide both sides of the equation by 3. Dividing the left side by 3 gives me (3x)/3, which simplifies to just x. On the right side, dividing 15 by 3 results in 5. So now my equation is simplified to x = 5.\\n\\nWait a second, let me check if that makes sense. If I plug x = 5 back into the original equation, does it hold true? Let's see: 3*(5) + 5 equals 15 + 5, which is 20. And the right side of the equation is also 20. So yes, both sides are equal when x is 5. That means my solution is correct.\\n\\nI think that covers it. I isolated x by first undoing the addition and then dealing with the multiplication. It's important to remember the order of operations in reverse when solving equations because you want to do the opposite operations in the right sequence.\\n</think>\\n\\nTo solve the equation \\\\(3x + 5 = 20\\\\), follow these steps:\\n\\n1. **Subtract 5 from both sides** to isolate the term with the variable:\\n   \\\\[\\n   3x + 5 - 5 = 20 - 5\\n   \\\\]\\n   Simplifying this gives:\\n   \\\\[\\n   3x = 15\\n   \\\\]\\n\\n2. **Divide both sides by 3** to solve for \\\\(x\\\\):\\n   \\\\[\\n   \\\\frac{3x}{3} = \\\\frac{15}{3}\\n   \\\\]\\n   Simplifying this results in:\\n   \\\\[\\n   x = 5\\n   \\\\]\\n\\nTo verify the solution, substitute \\\\(x = 5\\\\) back into the original equation:\\n\\\\[\\n3(5) + 5 = 20\\n\\\\]\\nSimplifying:\\n\\\\[\\n15 + 5 = 20\\n\\\\]\\nWhich confirms that \\\\(x = 5\\\\) is the correct solution.\\n\\n**Final Answer:** \\\\(x = 5\\\\)\", additional_kwargs={}, response_metadata={'model': 'deepseek-r1:1.5b', 'created_at': '2025-05-18T17:45:02.231559364Z', 'done': True, 'done_reason': 'stop', 'total_duration': 6206604042, 'load_duration': 2810592821, 'prompt_eval_count': 18, 'prompt_eval_duration': 313029052, 'eval_count': 681, 'eval_duration': 3081243306, 'model_name': 'deepseek-r1:1.5b'}, id='run--3b5ae008-b3a4-4664-847d-31f5be16c143-0', usage_metadata={'input_tokens': 18, 'output_tokens': 681, 'total_tokens': 699})]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is Josh\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
    ")\n",
    "chat_map[\"id_123\"].messages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
