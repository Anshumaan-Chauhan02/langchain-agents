# ðŸ§  LangChain + Ollama Local Agentic Systems

> Build, customize, and experiment with LangChain-powered agents running local LLMs through Ollama!

---

## ðŸ“˜ Overview

**LangChain** is a powerful Python framework for building agentic systems using LLMs. It enables abstraction of components, making it easier to construct agents, chains, memory-aware conversations, and more. This abstraction extends across the LangChain ecosystem including:

- **LangGraph**: Graph-based reasoning framework.
- **LangSmith**: LLM observability and debugging tool.
- **LangFlow**: Visual editor for LangChain apps.
- **LangServe**: Server-ready LangChain deployments.

In this project, we'll walk through setting up a LangChain project using the **Ollama** Python library to run LLMs locally and customize agent behavior.

---

## ðŸš€ Project Setup

We'll use [`uv`](https://github.com/astral-sh/uv), a fast Python package and environment manager written in Rust.

### ðŸ§° Installation Steps

1. **Initialize the project**:
   ```bash
   uv init langchain-agents
   ```

2. **Create a virtual environment**:

   ```bash
   uv venv --python 3.12.7
   ```

   ðŸ‘‰ To see all Python versions on your system:

   ```bash
   uv python list
   ```

3. **Activate the virtual environment**:

   ```bash
   source .venv/bin/activate
   ```

4. **Install dependencies**:

   ```bash
   uv add \
     langchain-core langchain-ollama langsmith docarray \
     langchain-community ipykernel langchain \
     fastapi uvicorn google-search-results ollama
   ```

5. **Install Ollama**:

   * Follow the [official installation guide](https://github.com/ollama/ollama-python).
   * **If using WSL**:

     ```bash
     curl -fsSL https://ollama.com/install.sh | sh
     ```

6. **Pull a model** (e.g., Qwen 0.6b):

   ```bash
   ollama pull qwen3:0.6b
   ```

7. **Test the setup**:

   ```bash
   uv run ollama_basics.py
   ```

---

## ðŸ§ª Ollama Configuration Tips

ðŸ§° You can find more advanced model options and configuration settings in the [Ollama REST API Docs](https://github.com/ollama/ollama/blob/main/docs/api.md).

![Ollama Settings](docs/imgs/ollama_model_options.png)

---

## ðŸ–¥ï¸ VS Code + WSL Quick Fixes

* âœ… **Interpreter selection**:

  * `Ctrl + Shift + P` â†’ "Python: Select Interpreter"
  * Enter path: `.venv/bin/python`, or use `python3` or `python3.12`

* ðŸ”„ **Reload Window**:

  * `Ctrl + Shift + P` â†’ "Reload Window"

---

## âš ï¸ Model Selection Note

We're using non-instruction-tuned models like **DeepSeek-R1** and **Qwen3**, which can lead to unpredictable or incoherent outputs.

ðŸ‘‰ Itâ€™s **highly recommended** to use **instruction-tuned models** like **Llama3** for more reliable and contextual responses.

---


### ðŸ§± LangChain Core

Foundation layer that provides abstractions for:

* Prompts
* Memory
* Chains
* Tools
* Agents

---

## âœï¸ Prompt Basics

LangChain recognizes three prompt types:

1. **System Prompt** ðŸ§ 
   Provides role and behavior for the LLM (e.g., â€œYou are a helpful assistantâ€¦â€)

2. **User Prompt** ðŸ‘¤
   Carries the task or question from the user.

3. **AI Prompt** ðŸ¤–
   LLMâ€™s generated response to the user.

LangChain offers templates to compose and manage these prompt types effectively.

---

## ðŸ§ª LangSmith

A debugging, testing, and observability tool for LLM applications.

* Monitors how your agents behave
* Tracks prompts, outputs, and traces
* Requires an API key (free tier available)

---

## ðŸ’¬ Prompting Strategy

An effective prompt contains:

1. **Rules**: Behavior definition for the model.
2. **Context**: External info (a.k.a. Retrieval-Augmented Generation).
3. **Question**: What the user wants.
4. **Answer**: What the LLM returns.

---

## ðŸ§  Chat Memory in LangChain

LLMs donâ€™t retain previous inputs by default. LangChain provides memory wrappers to retain conversation history.

### ðŸ§  Memory Types

| Type                              | Description                                 |
| --------------------------------- | ------------------------------------------- |
| `ConversationBufferMemory`        | Stores all past messages in memory.         |
| `ConversationBufferWindowMemory`  | Retains only the last N messages.           |
| `ConversationSummaryMemory`       | Summarizes entire conversation history.     |
| `ConversationSummaryBufferMemory` | Mix of buffer + summary using token limits. |

âš ï¸ Some of these are deprecated â€” prefer using `RunnableWithMessageHistory`.

---

## ðŸ›  Agents in LangChain

Agents enhance LLMs by allowing tool usage (e.g., search, calculators, databases).

ðŸ”§ **Key points when defining tools**:

* Use docstrings to explain when and why the tool should be used.
* Provide self-explanatory argument names.
* Add clear type annotations for both input and output.

Example:

```python
def get_weather(city: str) -> str:
    """Gets current weather for the provided city."""
    ...
```

---

## ðŸ§¬ LangChain Expression Language (LCEL)

LCEL allows composing LLM chains declaratively and functionally â€” similar to RxJS or functional streams.

âœ… Ideal for:

* Chaining prompts
* Handling memory
* Creating reusable LLM components
